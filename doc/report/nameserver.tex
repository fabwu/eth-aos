\chapter{Nameserver}

The system, we have built up to the last group milestone, was not 
entirely ready to take full advantage of a nameserver. The core of our OS
has basically one server domain, which is called not surprisingly \verb|init|, 
and handle all services available to the other domains. Although there were
many opportunities to split up the system, we always opted for a centralised 
solution due to time constraints and other implementation issues.

As a consequence of these design choices, a domain was only able to send messages 
to init but not to other domains. So before implementing the nameserver, I had to
extend the existing RPC infrastructure. These changes uncovered more issues with
the existing code (process management), which also had to be addressed.

After that, I used the existing messaging functionality to implement a nameserver
with a separate RPC call using frames for transmitting the payload. All changes
to the core system and the design decisions I made, when I was implementing the
nameserver, are described in this chapter.

\section{Changes to Existing RPC Infrastructure}

In our core system all RPC requests are served by \verb|init| and there are no separate
domains for e.g. a memory server. Every domain has one LMP channel to \verb|init|, which
is used to send a request. After sending a request the domain registers a closure on the
channel, which get called when the response from \verb|init| arrives. Chapter 3 has more
details about how a domain communicates with \verb|init|.

For this section the details of the existing RPC system are not important, but I want to
point out two aspects, which have driven most of the design decisions in this chapter. 
First, there was no existing functionality to setup direct channels between two domains.
While channel setup is relatively straight forward with LMP, it is quite a challenge to
setup UMP channels (as we cannot transfer capabilities between cores).
The second aspect is that the existing RPC calls must work until the nameserver is fully
implemented and some calls might be never replaced.

Considering the points above a multi-hop protocol over \verb|init| seems to be easier
to implement in the setting of this course. In the initial design each domain has a
LMP channel to \verb|init| and the first word of a message is used for metadata (message
type, sender, receiver). 

If a domain wants to send a message to another domain, it has to create the correct header 
using an identification for the receiver (e.g. PID). The sending domain forwards the 
message over the LMP channel to \verb|init|. \verb|init| receives the message and checks
if the receiver is one the same core. If the receiver is on a different core, \verb|init|
forwards the message over UMP to the \verb|init| process on the other core (UMP channel 
already exists). If \verb|init| is the receiver, the sender used the RPC calls from 
the core system to send the message. \verb|init| handles the message and sends the 
response back. If the receiver is not \verb|init| but on the same core, \verb|init| looks
up the LMP channel of the receiver and forwards the message.

The advantages of this multi-hop protocol are that it requires no direct channel setup and
communication with the second core over UMP is also straightforward. Furthermore, \verb|init|
could enforce certain security policies (e.g. sender is correct, block certain messages) and
the RPC calls from the core system still work. However, this approach has also its drawbacks. 
As all messages are routed over one domain, performance could suffer as \verb|init| becomes 
a bottleneck. Another issue shows up on the receiver side, because several messages from different
clients arrive over one channel. If we want to support requests which are split into several message,
the receiver needs a demultiplexer and a message buffer. Section \ref{ns-service-call} describes
how this problem was solved for the nameserver.

The existing RPC system already encodes the message type into the first word of a LMP message (8-bit for
message type, 8-bit for message sub type). For the multi-hop protocol the first word has to include additional
information required for routing. First, we need an ID to identify the sender and receiver of a message.
The PID of each domain is the perfect candidate for this ID, because our spawn code already encodes the 
core id into the PID. On Barrelfish the PID uses 32-bits but we have only 64-bits for the header available.
This is enough for the sender and the receiver, but we couldn't encode any additional meta-data required for
the existing RPC system. Thus, I decided to limit the PID to 16-bits and use the first 4-bits (msb) to encode
the core id. This limits the number of concurrently running processes to 4096, which is obviously insufficient 
for a real-world OS. Furthermore, we don't reassign PIDs from terminated domains but this could be easily 
implemented with a free-list for the PIDs. Nevertheless, this limitation shouldn't affect the small system 
we build for this course. Figure \ref{fig:ns-header} shows how the complete header is specified.  
\begin{figure}[h]
\centering
\begin{bytefield}[bitwidth=0.5em]{64}
    \bitheader[endianness=big]{0,8,24,40,64} \\
    \bitbox{24}{unused}
    \bitbox{16}{receiver PID}
    \bitbox{16}{sender PID}
    \bitbox{8}{type}
\end{bytefield}
\caption{Message header}
\label{fig:ns-header}
\end{figure}

Now \verb|init| can easily extract the receiver of a message from the header using macros. In order to forward
message \verb|init| has to lookup the LMP channel of the receiver. The LMP channel from init to each domain is
stored in the \verb|spawninfo| struct, which gets created when we spawn a domain. Unfortunately, the struct is
discarded after the domain was spawned so there is no way for \verb|init| to lookup a channel based on the PID.
Additionally, a process manager was running on each core so we didn't have a complete view of all processes
running on the system. I introduced a spawn manager per core which holds the state of the domains (e.g. LMP channel)
which were spawned locally. Then there is one process manager on the boot core, which maintains a list of all
processes running on the board. After these changes \verb|init| can lookup the LMP channels of the domains that
were spawned on the same core or forward the message over UMP to the other core. At this point a domain can send
a message to another domain, which receives the message using a closure.

While testing the message passing it quickly turned out that one channel to \verb|init| is not sufficient. The receiver
has to register a closure on the channel to get messages from other domains. But this creates a conflict with the 
existing RPC implementation, which also registers a closure when waiting for the response from \verb|init|. It became
apparent that several channels to init are required, so I simplified the creation of channels from init to its child
domains. After this change I had all the necessary functionality to proceed with the implementation of the nameserver.

\section
{
    Architecture
}
\label{ns-architecture}

The nameserver is running in a separate process and is the first process that get started after \verb|init| is ready. This makes
sure that the nameserver always get PID \texttt{0x1}. If a newly spawned process wants to contact the nameserver, he can just
set PID \texttt{0x1} in the receiver header field, which makes bootstrapping the initial connection to the nameserver trivial.

If a service isn't available when a domain starts up, it just sends lookup requests in a loop until the service is ready.

I decided relatively early in the design phase against direct channels between domains. In retrospect, this turned out too make
the implementation way more complicated than it would be with direct channels between domains. The initial RPC system split up large
messages at the sender side and then \verb|init| reassembles them in a closure. Now if a domain wants to offer a service, it registers
a closure on the channel to \verb|init| and \verb|init| will forward all incoming messages for this service to the domain. If different
clients split up large messages, they don't arrive in the correct order at the server. This requires buffering and demultiplexing
on the server side, which was something I wanted to avoid. I took another shortcut (which I should avoid in the future) and restricted
the protocol in a way such that a request to a server fits into one LMP message (i.e. 3 machine words) and the response also has to 
fit into one message. Large message can then be transferred using a frame and the LMP message contains just the address and size of
the frame. This protocol avoids multiplexing but allocating and especially freeing a frame adds additional complexity to the 
implementation. Another drawback is that allocating a frame is relatively slow and the smallest granularity for our buffer is 4KB,
which is probably too large for most messages. We noticed this performance penalty, when we tried to implement the shell, which
sends a message (and allocating a frame) for each character that is printed to the console. We could avoid this performance issue
by not allocating a frame if the message payload fits into the LMP words, but there were other problems coming up, so the shell
still uses the old RPC calls and this performance improvement was not implemented.

As mentioned before, one channel to \verb|init| wasn't sufficient so the final design ended up with three channels to \verb|init|:

\begin{itemize}
    \item \verb|aos_rpc_chan|: This channel is used for RPC calls from the core system (\verb|aos_rpc.c|)
    \item \verb|ns_client_chan|: This channel is used if the domain acts as client, requesting a service from another domain using 
        \verb|nameservice_rpc|
    \item \verb|ns_server_chan|: This channel is used to register a handler, if a domain wants to offer a service using 
        \verb|nameservice_register|
\end{itemize}

With this setup a domain can do blocking calls to the existing RPC handler running on \verb|init|. Additionally, a domain can act
as server and do blocking calls to other services i.e. acting as a client. At the moment the nameserver is implemented using low 
level LMP functions, but with this channel setup the nameserver could register his own services (e.g. return list of services)
using the same API (in \verb|nameservice.h|) as the other domains.

The router running on \verb|init| registered different handlers on each channel. The handler on the \verb|aos_rpc_chan| is the
same as in the core system. He does a check that the message receiver is \verb|init| and then executes the existing protocol.
The handler registered on the \verb|ns_client_chan| checks if the receiver is on the same core. If not he forwards the message
over UMP to the other \verb|init| process. If the receiver is on the same core he looks up the \verb|ns_server_chan| for the
receiver domain and forwards the message over this channel. The handler listening on the \verb|ns_server_chan| forwards the
message to the \verb|ns_client_chan|.

After this general overview over the nameserver design, the next sections describe the actual protocol in more depth.

\section{Service Registration}

A domain can register a service by providing a name, a function pointer and some state to the \verb|nameservice_register()| function.
The service name can be any string but the length is limited to 192 characters because it has to fit into the three LMP words.

When a domain calls \verb|nameservice_register()| for the first time, the function initialises a hashtable and register a closure
on the \verb|ns_server_chan|. The closure is responsible for receiving incoming messages and calling the correct handler. After
the initialisation completed, a message is sent to the nameserver which contains the name of the service. The nameserver checks
if the name is free and stores the PID of the server together with the name in a hashtable. Then he reports the status of the 
operation back to the server. The messages are passed over the client channel to \verb|init| and from init over the server channel
to the nameserver and back over the same channels. If the requesting domain is on a different core, the message is routed over UMP.

If the name is available the requesting domain adds an entry with the callback and state to the local hashtable. The service is now
ready for receiving messages.

Removing a service follows a similar protocol, but the nameservice also checks if the request comes from the correct PID. As a malicious
domain cannot forge the sender PID (init takes care of that), this simple check ensures that only the domain who registered the service
can actually deregister it (considering that the PID was not reassigned to a different process in the mean time).

\section{Service Lookup}

Now if a domain wants to use a service, it first has to get a channel with \verb|nameservice_lookup()|. The protocol is similar to
service registration. The name of the required service is sent to the nameserver, which performs some checks and sends a response back.
If the lookup was successful the nameserver returns the PID of the domain that offers the service. Then the library allocates a 
\verb|struct nameservice_chan|, which holds the PID and the service name. The caller gets an opaque pointer to this struct back, which
he can pass to \verb|nameservice_rpc()| to make a request.

The function \verb|nameservice_lookup_did()| is a wrapper around \verb|nameservice_lookup| and returns the PID instead of the channel pointer.
This function is used by the shell command \verb|nslookup|.

At this point, the client obtained a channel to the service, he is ready to issue a request.

\section{Service Call}
\label{ns-service-call}

A client can contact a service with the \verb|nameservice_rpc()| function, send an arbitrary request and get a response back. The nameservice
library casts the opaque channel pointer back to the \verb|struct nameservice_chan| to get the name and PID of the service. 

As mentioned in section \ref{ns-architecture} we restricted the protocol in such a way that we cannot send multiple messages back and forth 
for one request. The only possibility to transfer arbitrary large payloads is to allocate a frame and transfer the capability to the other 
domain. We cannot send capabilities across cores but we can forge them using \verb|frame_forge| but I cover UMP communication later on.

After getting all the necessary information to contact the service, the client library allocates a frame of size 
\verb|name_bytes + request_bytes|. The frame is then mapped into the client's address space and we get a pointer to a send buffer. The client
copies the name of the service followed by the request content into the send buffer. Then he creates a LMP message, addressed to the server,
which contains the frame capability and the size of the whole frame. It is not necessary to send the length of the service name or the size
of the request payload, as we can recover these values using the null character of the service name. This message is now forwarded over the
client channel to \verb|init| for routing.

When \verb|init| receives the message, it checks if the receiver is on the same core. \verb|init| has to take some additional steps for 
cross-core communication, so the case where the client and the server are on the same core is explained first. As the server is on the same
core, we can simply forward the message, including the frame capability, to the server channel.

When the message arrives on the server side, a \verb|nameservice_handler()| closure is called, which handles the message. This closure was
registered previously on the server channel, when the server called \verb|nameservice_register()| for the first time. The nameservice library
parses the message for the client PID and the size of the frame. Then the frame, which the server got over LMP, is mapped into the server 
domain's address space. Now the nameservice library reads the service name from the beginning of the frame and determine the size of the 
payload using the null terminator. Then we check the hashtable if a closure is registered under the given service name. If not we clean up
and return an error to the client. If the lookup was successful, we call the closure with the state in the hashtable and the message payload.
The service is now doing his work in the closure and returns a response. The nameservice library allocates another frame as return buffer and
copies the service name followed by the response into the buffer. The service name in the return buffer could be omitted, but it acts as
sentinel on the client side (received response from correct service) and makes sure that the frame size is not zero. Now the capability and
size of the response frame is sent back to the client. On the client side the frame is again mapped and the response is copied into the buffer
provided as an argument. This concludes a complete RPC request on the same core.

Before I go to the inter-core RPC I want to briefly discuss the clean-up of the allocated frames after a request is completed. The clean-up
is a bit tricky because both sides have to allocate resources (e.g. client doesn't now size of response from server). Now if for example the
server frees an allocate frame too early, the client doesn't receive the complete response. The easiest solution would be a two-stage protocol.
The client alone is responsible allocating and freeing up resources. Before the server sends back a response, he would tell the client the
size of the response and get a frame in return. If the client has finished up, he can free both the request and the response frame.
Unfortunately, this requires sending multiple messages back and forth, which is not possible with the current design. Another solution, is to
allocate on one side and free on the other side (e.g. server allocates response frame, server deletes cap and client frees response frame).
This approach doesn't require multiple messages but due to limitations of the memory manager \verb|frame_free| cannot free resources
of other domains. I didn't have time to investigate this issue further so the current implementation is leaking memory.

If the client and the server are not on the same core, \verb|init.0| forwards messages without a frame over the existing UMP channel to 
\verb|init.1| on the other core. \verb|init.1| looks up the LMP channel of the receiver and forwards the message accordingly. If the 
message contains a frame capability i.e. has a payload, \verb|init.0| finds the physical address of the frame and sends a message 
with the address and the size to \verb|init.1|. On the other core, \verb|init.1| forges a frame capability and sends it to the receiver
via LMP. The advantage of this approach is that the shared frame acts as a pseudo direct channel between the client and the server, so
the UMP channel isn't congested with small messages from a fragmented frame. What's more, the nameservice library can be reused on the
second core without any changes, as the communication still goes over LMP. A huge drawback is that we cannot free the frame, after it
was used, which leads to more memory leaking. Another issue is that this solution cannot exploit the cache coherence trick, which uses UMP
to speed up message transport. As I implemented this approach I was a bit surprised that the other process, can see the content of the frame,
despite ARMs weak memory model. But then I discovered that the UMP queue inserts a data memory barrier, when a message is enqueued. This
makes the payload frame available on the other core without any additional effort from the nameservice library.

There is one remaining issue with shared frame transfer. As LMP can only transfer one capability per message, we can either send a large
payload (slot is used by frame capability) or a capability. As we cannot transfer capabilities between cores and I didn't come to the point,
where I migrated other RPC services (e.g. memory server) to the nameservice API, I omitted this implementation.

\section{Shell Integration}

For this project we had to implement two shell commands to interact with the nameservice. The first is \verb|nslookup <service_name>|, which
simply prints the PID of the domain which offers the service. I implemented this command using the \verb|nameservice_lookup_did()| function
described earlier.

The other command is \verb|nslist| that prints all registered services. When the nameserver receives a message from the nameservice library,
he simply prints the hashtable.

\section{Migration of Existing Services}

Despite several issues with the nameservice library, a fellow team member successfully implemented the network project with it. The
networking has reasonable performance and works also across cores. Sadly, the file system and the shell didn't use the nameservice
library. One feature, that was also requested from the network project, was an asynchronous API for the client. Consider the case
where a shell wants to receive characters from a terminal server. It would be convenient for the shell to register a callback at
the terminal server, which gets called if a character arrives from the serial port. One can implement this scenario with the current
API (the shell has to act as server) but it is not cumbersome and not very intuitive.

The other aspect, which also affected the migration of the existing services (e.g. get more memory), is that we simply run out of
time in the end and had to focus on the report.

In retrospective, I can say that not using direct channels between domains (at least for LMP), was definitely not a good design choice and
made the implementation of the nameserver more complicated that it should have been. At the point were I realised the problem with freeing
the resources I have already spend to much time on this design and I couldn't work out a better solution in this limited time frame. 
Nevertheless, I learned a lot while implementing the nameserver, because I touched many parts of the core system again and had to adapt them
to the current design.
