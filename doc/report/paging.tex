\section{Address manager}

\subsection{How/Datastructures}

The addr manager does what the memory manager does for physical addresses for
virtual addresses.

It is initialised by giving it a minimal addr, and a max addr.
This means that it doens't need to handle physical representation (ram
capabilities), so the bi linked list and origin references are not needed anymore.

But it needs to additionally allow for requesting specific virtual addresses
(\verb|addr_mgr_alloc_fixed|).
To allow for this, we not only need have an avl tree for free ranges indexed by
size, but additionally another avl tree with the same free ranges indexed by
their base address.
For that wee need to find the range with largest base address, which is smaller
or equal to the requested range base address.
We then need to check if the range from the requested base address onwards is
large enough, and carve out the desired range by splitting it to two times into
smaller ranges.

To allow for checking if a virtual address is really in use, or this is just
dereferencing a stray pointer, we also allow to query if there exists an
allocated range for an address (\verb|addr_mgr_is_addr_allocated|).

With that, we have all the functionalities needed by paging.

\subsection{Limitations}

As should be obvious by now, the code of the address manager and memory manager
is mostly duplicate, it should be possible to roll it into some general
abstraction used by both paging and memory manager.

\section{Paging}

Paging manages the page table, and in doing that interacts with the address
manager to check if what it is going to page is valid.
Only after paging is memory given out by the memory manager really usable to
applications, as they are normally not allowed to access physical memory
directly.
We already see that paging and memory manager are interdependent, as
paging can't page without getting memory from memory manager, and the memory
manger can't give out memory without having its buffer paged in.

\subsection{How}

Paging is intialised by calling \verb|paging_init|.
As we don't get passed through any of the page tables capabilities for the
addresses already paged, we need to set the start address of the address manager
to the second slot of the l0 page table.
This ensures we don't overwrite the first slot of the l0 page table, wiping out
our existing virtual adress space.
We hope that $2^39$ bytes should be enough for the existing address space, it
doesn't use any of the other l0 slots except for the first.

We then call \verb|paging_init_state|, which stores the l0 page table capility,
initialises the mutexes, and slabs, and also initalises the address manager.
We then grow the slabs with slabs with static buffers, as for the first few
pagings we can't depend upon memory manager, we would get into a circular
dependency.

\subsection{Datastructures}

\begin{displayquote}
Turtles all the way down.
\end{displayquote}

We describe here how we implemented the shadow page table.

To note here is that we chose not to go for a one to one shadow page table.
This because we not only need to store the address of the next lower level
shadow page table, but also the mapping capability, the table capablity. So we
wouldn't fit into on page, which would have made things quite complicated,
inefficient.

\subsubsection{History}

Originally, every thing was doubly linked list (a bit of recurring theme). 
\begin{itemize}
	\item l0 would be a paging node without a parent, its page table capability,
				and a doubly linked list of its children.
	\item l1 would be paging node with l0 as a parent, its page table and mapping
				capability, and a doubly linked list of its children.
	\item l2 would be paging node with l1 as a parent, its page table and mapping
				capability, and a doubly linked list of its children.
	\item l3 would be paging node with l2 as a parent, its page table and mapping
				capability, and a doubly linked list of its children.
	\item l4 would be paging node with l3 as a parent, without page table and mapping
				capability, but instead it's page table capability would be set to the
				frame capability. Also, it's children would also be \verb|NULL|.
\end{itemize}

Every level except for l0 has a slot assigned to it. Also we store the level at
which the paging node is, making it easier to deduplicate code, handle all the
different levels mostly the same.

The address manager didn't exist yet. Therefore in \verb|paging_map_frame_attr|
there would just be a static counter initialised to zero, incremented according
to how many pages need to be mapped to fulfill mapping request.

\subsubsection{Current State}

In the current state we replaced the doubly linked list with avl trees, but
keeping the same overall structure, code.
This gives quite a speed up, as instead of invoking four times an $\mathcal{O}(n)$
operation (keeping in mind that n is bounded by $2^9$), we have four times
$\mathcal{O}(log(n))$. For the worst case ($n=2^9$), this gives a speed of up
two magnitudes for mapping one pag (it would be obvious for the code to not
retraverse the wole page table when mapping multiple pages at once, but as we
will see in \ref{pag-con}, this is not feasible, so we hit this speed up for
ever \verb|BASE_PAGE_SIZE| of bytes mapped).

And we switched out the static counter for the address manager, as one simple
counter wouldn't fulfill the requirements anymore (see later chapters).

\subsection{Reentrancy and Concurrency} \label{pag-con}

First to note again that we need to break up the interdependency of paging and
the memory manager, e.g. MÃ¼nchhausen again.
This means paging needs to be able to page without the need to page in internal
datastructures.
This means memory allocator needs to be able to serve memory without the need to
use memory for internal datastructures.
That's why both use slab allocators, which we refill when we are below a
watermark, and have bit that tells us if we are already refilling up again, to
not double refill, get into an infinite recurse.

And we might get quite deep dependency chains, with for example paging wanting
to refill its slabs, because of that memory manager needing to allocate a slot,
the slot allocator needing memory to expand the cspace, and finally the memory
manager needing to page because of of it refilling its slabs, needing to page in
memory for that (probably not even the deepest it can go).

As in memory manager, we also have just emperically decided for a
reserve/watermark of 32 for the slabs.

But importantly this is bounded number, we can't have unlimited reserve
ressources.

Paging might also get called from multiple threads (see later chapters),
compared to the memory manager, which is always single threaded, only reentrancy
can lead to concurrency.
Also to note is we allow for unmapping, so page tables might also get delete.

We use locks (now they are not just canary locks, they are really needed because
of multiple threads) in \verb|paging_map_some| when adding to a page table.
Before the lock, all preparations that might trigger reentrancy are already
aquired.
So that in the region of the lock, there won't ever be any concurrency.
This is needed.
As an example, if we add to the same slot, say of a l1 page table, we will
have duplicate keys in the avl tree, breaking it, as it operates on the
assumption that keys are distinct.
Also we break the page table, as it is now undefined which mapping will be left
intact.

We allow for unmapping, and also use the same lock when manipulating page
tables.

When we now introduce that page tables also might get deleted, the page table we
are about to add to might just have been pulled from under our feet.
For that, we would needed to have a critical regions that spans the full page
table walk, which we currently don't have.

But we also have upper bound for the critical region which are sure about.
Locking of the whole map would lead to an unbounded size of the
critical region (potentially unbounded many pages to map), resulting in the need
for unbounded amounts of reserve ressources.
So if we would wan't to chache page table results, we would need to discern
between the total mutual exclusion of map and unmap, and the partial mutual
exclusion between two mappings, that can be broken to a specific page table
level.
But we might also starve unmap if we would implement it this way.

\subsubsection{History}

Initially, the counter was used as is, and increment after paging.
As one can imagine, if we trigger reentrancy by refilling the slabs, we did
overwrite what we just did write, leading to interesting bugs.
This was one of the first encounters with reentrancy, as it is always triggerd
on the first paging, and was hard to reason as one didn't know yet what was one
dealing with.

That's why the counter is read into a temporary variable, then incremented, only
then we call page in.
This guarantees that the counter values for the different paging calls are
disjunct.

\subsection{Limitations}

As everything concurrency in paging is quite tricky, we did not do certain
things which would have potentially made more correct, faster.
For example, the page table walk results should have been cached for the
additive case that is map, the code could already handle that case.
Paging map and paging unmap should be mutual exclusive. As it is there is still
the possibility that concurrent running paging unmap might preempt a paging map
call, or even thrash the datastructures.

We don't support mapping of huge pages, which could give quite some speedup, as
significantly less context switches.
