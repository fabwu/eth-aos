\chapter{Network}

\section{Architecture / Design}

\subsection{Running the Network Driver}

One of the earliest decision in the network project was, where the driver should run. One of the considered options was, to start an additional core that would be dedicated to run the network driver and handle the network protocols. The advantages of this approach would be:
\begin{itemize}
    \item Better performance and lower latency: The network could run uninterrupted and be preempted much less and only for the kernel. (This is an assumption, as we did not try to implement this approach and measure it.)
    \item Fast client applications that use networking: Because the networking would run on a different core than any other application, UMP could be used as the sole message passing system. This would not only make the interface simpler but also faster, as the book has already shown that UMP is faster than LMP.
\end{itemize}
The disadvantages would be:
\begin{itemize}
    \item Refactoring multi-core memory management: As detailed in previous chapters, we split the memory between the two already running cores. We agreed as a team, that it would not make much sense to split the memory three ways for the network and it is hard to predict which core would need how much memory. So for a networking core to be functional we would have to implement a way to pass memory to the networking core, which we previously decided not to do because we deemed it to be a lot of complicated work.
    \item Unrealistic: If we were to continue work on this OS we would at some point want to start all the cores and use them for user applications. At this point we would not want a core dedicated to networking anymore. When looking at the bigger picture, it therefore does not seem to make sense to have a dedicated networking core and would also feel like cheating to be able to show better performance in the final report.
\end{itemize}

Considering the arguments above we decided to run the networking driver in its own domain which is pinned to core 0.

The idea of running the driver in the main process was quickly discarded as we already consider that to be a too big monolith that we would like to trim down. Also the idea of running the driver on multiple cores sounds like a nightmare and was not further considered.

\subsection{Running the Network Protocols}

The next decision was where to run the code for the different network protocols and how they would interact with the driver and each other. The book already suggests adding the protocol specific code to the driver application. This comes at a cost of flexibility and modularity, but is way simpler and faster to implement. additionally the nameservice project was not started at this point, so the communication between protocols would have to be improvised and be rewritten later on.

With those arguments in mind, the decision to implement the protocols inside the driver application. This also allowed to defer the implementation of the communication with other applications until nameservice made some progress, which was in retrospect a great time saver.

\subsection{General Design Decisions}

One of the design goals in the networking project was to reduce copying of data and therefore passing a reference into the same ethernet frame between the network protocol. Another goal was to keep the number of mallocs low and allocate on the stack whenever possible. This should simplify resource management and reduce the amount of bugs.

An important design decision was to keep the interface between the network protocols simple, straight forward and without a lot of abstraction. This makes the network stack less extensible and violates the open-closed principle when e.g. adding a new protocol. The advantages are, that the its faster to implement like this and there is no overhead caused from the abstraction.

That being said, we made it a goal to separate the code for the different protocols: Each network protocol has its own files and a clearly defined interface for interacting with other protocols. For example the IP protocol has its own header and source file and all the IP code is inside those files. When calling functions to send IP packages, the code of the IP protocol will call functions of the ethernet implementation to send IP packages over ethernet frames.

\section{Driver}

The driver to interact with the networking hardware was given in the handout. Because its functionality was not changed, the driver itself will not be covered in the report but only the interaction with it.

At startup the driver is handed two large frames: One to write incoming ethernet frames to and the other to read outgoing ethernet frames from. The driver uses slices that are half a page each per ethernet frame. So that a page is used for up to two ethernet frames.

To interact with the driver enqueue and dequeue operations are used. To get access to received ethernet frames the dequeue operation is used. It gives an offset into one of the memory regions used and blocks that part of the memory until it is released back to the driver. To do so the enqueue operation is used. Transmitting outgoing ethernet frames works in a similar way. The client of the driver has to keep track on which frames can be written to. Ethernet frames can be handed to the driver with an enqueue operation. To get back access to the memory after the sending is done, the dequeue operation has to be called again.

Because the enqueue and dequeue operations do not give a virtual address or pointer to work on, but only an offset to the base of the large frames passed to it, we decided to map those frames again a second time for use in the protocols. This we operate on different virtual addresses for the same physical memory in the same domain. The advantage of this was, that the queue interface used by the driver had not to be changed or rewritten.

\subsection{Disabled Caching}
A big problem early on was, that parts of the sent ethernet frames were sometimes not correct. On closer investigation the wrong parts were identical to earlier ethernet frames (e.g. sending an ICMP echo reply contained bytes from an earlier sent ARP request). After suspecting a caching problem, the memory used to interact with the driver was mapped with deactivated caches and this solved all of these problems. Later on we discovered that the same was suggested on the moodle forum to other groups, so we did not search for an alternative. But we suspect that this is currently the main bottleneck in the networking code.

\subsection{Copy of Data}
Earlier we said that a design goal was to keep the copying of data to a minimum. But after seeing how the ethernet driver worked we decided to not share to which the driver wrote. The main reason for this was a security concern: The minimal amount of memory that can be shared is one page. But one page holds two ethernet frames. Because two ethernet frames sharing a page might not belong to the same client it would be unsafe to share the memory, as then a client could get access to data that belongs to another client. This could even be forced by producing a lot of traffic.

It would be maybe possible to rewrite the driver to handle it differently, so that the memory could safely be shared with clients. But we deemed this to be to big of a task for the limited time available.

So we had to add at least one copy to send the data to the client. But we made sure it was minimal, by only copying the payload, but none of the protocol headers. Because we wanted to send some additional data like the ip address and port numbers, those were copied to the back of the memory, right after the payload. This can be safely done, because the memory reserved per ethernet frame by the driver is substantially bigger than an ethernet frame could be. The driver does this, so the ethernet frames are aligned to half the page size.

\section{Ethernet}
The ethernet handling was implemented in \verb|usr/drivers/enet/ethernet.c| and its interface defined in \verb|usr/drivers/enet/ethernet.h|.

\subsection{Receiving Frames}
Receiving frames was rather straight forward. The main event loop in \verb|usr/drivers/enet/enet_module.c| is constantly checking for incoming ethernet frames. When receiving an ethernet frame (over driver dequeue), it calls the ethernet handle function which is blocking. As soon as this function returns the memory slice in which the incoming ethernet frames resided in was released again by calling enqueue on the driver.

The handle function would look at the ethernet frame and call the IP or ARP handle functions respectively. The ethernet frame would be discarded and not handled if the protocol was unknown or the destination ethernet address was invalid. The address would be considered invalid if it was not a broadcast address or the ethernet address of the Toradex board.

\subsection{Sending Frames}
\label{sec:sendframe}
The main difficulty for this part was managing the memory regions for sending ethernet frames correctly and efficiently. We decided to create a linked list with nodes for each ethernet frame sized memory slice. If the node was in the linked list, the slice was not currently in use. Because we already know the amount of slices in advance the nodes could be allocated all at once and the linked list created when initialising ethernet.

What makes this approach performant is, that the list has never to be traversed but only ever the first element has to be touched in each operation: For reserving a slice that can later on be sent, the front element of the list has to be removed and its address remembered. To release the reserved slice again the remembered element had to be simply added back to the front of the list.

\begin{figure}
    \centering

    \begin{sequencediagram}
      \newthread{cli}{client}
      \newinst{eth}{ethernet}
      \newinst{drv}{driver}
  
      \begin{call}{cli}{start\_send()}{eth}{id, ptr}
        \begin{sdblock}{loop}{}
            \begin{call}{eth}{dequeue()}{drv}{offset}
            \end{call}
        \end{sdblock}
      \end{call}

      \begin{call}{cli}{write\_payload(ptr)}{cli}{}
      \end{call}

      \begin{call}{cli}{send(id)}{eth}{}
        \begin{call}{eth}{enqueue()}{drv}{}
        \end{call}
      \end{call}
    \end{sequencediagram}

    \caption{Sequence diagram for sending ethernet frames}
    \label{fig:ethsend}
\end{figure}

The interface to send ethernet frames was built on that approach. Figure \ref{fig:ethsend} shows a sequence diagram of said interface. It consist of a function to start sending an ethernet frame, which returns an id and a pointer. The pointer can then be used to write the payload of the ethernet frame (e.g. an ip package). When done writing, the client calls the send function with the id. This id can directly be used by the ethernet protocol to enqueue the correct memory slice to the driver.

Another important part is, that the memory is not immediately free to be used again, because the data has to be sent by the hardware before that. This is done in parallel and it would be a waste of performance to block on that. So we decided to keep the data reserved and return from the send function. How data is released again can be seen in the same sequence diagram \ref{fig:ethsend}. In the start send function, ethernet dequeues all slices that were done sending from the driver. To figure out which list nodes to add in the tracking of free memory slices we use some pointer arithmetic. By using the fact that all of the list nodes were allocated as one big chunk and the address of each list node therefore corresponds to the address of a memory slice, the address of the correct list node can be quickly calculated using the offset returned by dequeue.

\section{ARP - Address Resolution Protocol}
The ARP handling was implemented in \verb|usr/drivers/enet/arp.c| and its interface defined in \verb|usr/drivers/enet/arp.h|.

\subsection{ARP Cache}
\label{sec:arpcache}
The ARP cache was saved using the provided hashtable in \verb|lib/collections|. The reasoning behind this was, that we need a datastructure that can do the lookup of an address as fast as possible but does not have to be as fast to insert or remove entries. Hashtables fulfil exactly these criteria. Only the IP address to ethernet address lookup had to be implemented, so a single hashtable was sufficient.

Another decision was in which byte order to store the addresses. We decided to store the ethernet addresses in network byte order. The main advantage of this is, that in the majority of uses, the ethernet address was needed in network byte order anyways, and so the conversion did not have to be done for every IP package over and over again. Additionally ARP receives the ethernet addresses in network byte order anyways so conversion could be saved here too. The only place were the byte order conversion had to be done was in the cache printing.

\subsection{Interfaces}
The ARP layer is strongly coupled with the ethernet layer, as it uses the ethernet sending functions to send ARP responses and requests and ethernet calls the ARP handling function if it received packages of the ARP protocol type.

The ARP layer is additionally strongly coupled with the IP layer: It offers a function that allows sending ARP requests to the given IP address and it notifies the IP layer when a new entry for an IP address is added to the ARP cache. This notification is done, so ip packages that are waiting for an address resolution can be sent (more on that in the IP part below).

\subsection{Printing ARP Cache}
Another interface that the ARP layer offers is printing the ARP cache. We decided to not send a copy of the ARP cache to the shell and do the printing there, but to directly use printf functions inside the ARP layer to render the cache for the serial output. This was mainly done, because it seemed to be a simpler way of doing it in terms of how much effort it would take to implement. Sharing the memory of the ARP cache was not considered, because that sounds like a security nightmare. As mentioned before in \ref{sec:arpcache}, when printing we had to also convert from network byte order.

\subsection{Additional Features}
ARP was implemented to support the additional features of sending ARP probe packages and handling ARP announcements.

An ARP probe package is a request send out to the network to check if anyone uses the IP address that we would like to start using. After starting the ARP protocol an ARP probe is sent to the network, checking if our static IP (hard coded 10.0.0.2) is already in use. If ARP ever gets an external entry for that static IP address, it stops the network driver. In the future this could be expanded to then request another IP address from DHCP or from the user.

ARP announcements are ARP request sent on ethernet broadcast addresses to tell other network members, that they are using the announced IP address. By handling these announcements, the ARP cache can be populated without having to send ARP requests to these targets. But more importantly handling them again reduces the risk of an IP address conflict.

\section{IP - Internet Protocol}
The IP handling was implemented in \verb|usr/drivers/enet/ip.c| and its interface defined in \verb|usr/drivers/enet/ip.h|.

\subsection{Receiving Packages}
Receiving IP packages is tightly coupled with the ethernet, ICMP and UDP layers. The ethernet layer calls the IP layer handle function when receiving an ethernet frame with the protocol bytes set to IP. The IP layer then looks at the package and sends it on to the ICMP or UDP layers depending on the specified protocol.

The package is discarded in the IP layer if...
\begin{itemize}
    \item ...the version of the package was not 4 (for IPv4).
    \item ...the package was part of a fragmented package. (We did not implement handling fragmented packages.)
    \item ...the package was not sent to our static IP address and also not to one of the broadcast addresses.
    \item ...the package was for a protocol other than ICMP or UDP.
\end{itemize}

The checksum of the IP package is checked and a message is printed if the checksum was incorrect but the package was not discarded in that case. It would be easy to also discard a package in that case, but we decided against doing so for now, because we did not want to discard packages if we miscalculated the checksum on our end (which happened multiple times in early development).

\subsection{Sending Packages}
Sending IP packages is tightly coupled with the ethernet, ICMP and UDP layers too. Because of how the interface of ethernet has split the sending functions in two (see \ref{sec:sendframe}), we decided to make this transparent to the client and implement IP package sending in the same style.

So there are again two functions, one for start sending a package and one for actually sending it. The start sending function returns a pointer to which the package payload can be written. It is noteworthy that to reduce the amount of mallocs (which is one of the design goals) we make the client allocate the storage needed to identify the package between the two sending functions. This means that the client can allocate that on the stack.

The start sending function takes (beside others) a parameter with the target IP address. Before it can call start send ethernet frame it needs to know the target ethernet address. For that the ARP lookup function is called to lookup the ethernet address given the target IP address.

\begin{figure}
    \centering

    \begin{sequencediagram}
      \newthread{cli}{client}
      \newinst{ip}{IP}
      \newinst{arp}{ARP}
      \newthread{eth}{ethernet}
  
      \begin{call}{cli}{start\_send(ip)}{ip}{id, ptr}
        \begin{call}{ip}{lookup\_ip(ip)}{arp}{not\_found}
          \begin{call}{arp}{send\_arp\_req(ip)}{eth}{sent}
          \end{call}
        \end{call}

        \begin{call}{ip}{alloc\_store(ip, id)}{ip}{ptr}
        \end{call}
      \end{call}

      \begin{call}{cli}{write\_payload(ptr)}{cli}{}
      \end{call}

      \begin{call}{cli}{send(id)}{ip}{}
        \begin{call}{ip}{store(id)}{ip}{}
        \end{call}
      \end{call}

      \begin{call}{eth}{arp\_rep(ip, eth)}{arp}{}
        \begin{call}{arp}{notify(ip, eth)}{ip}{}
          \begin{sdblock}{loop}{}
            \begin{call}{ip}{send\_ip()}{eth}{sent}
            \end{call}
          \end{sdblock}
        \end{call}
      \end{call}
    \end{sequencediagram}

    \caption{Sequence diagram for storing IP packages until ARP concludes}
    \label{fig:ipsend}
\end{figure}

In case looking up the ethernet address failed, we did not just want to discard the package. So the following was decided: We store IP packages up to a maximal amount in memory until we learn their ethernet address. Figure \ref{fig:ipsend} shows how this works. After looking up the IP address and finding that we do not know the corresponding ethernet address we send an ARP request for the given IP address and use malloc to create a temporary storage for the IP package. The client can then write the payload without knowing, that the package can not be immediately sent. For the client both cases are the same.

When the ARP adds an entry for an IP address it notifies the IP layer. There all packages that are stored for the given IP address are sent to the given ethernet address and the local buffer gets freed up.

This system has still room for improvement: We never get rid of stored IP packages to a target that is not reachable and can therefore not respond to an ARP request. So to solve that we could for example stored and the IP package storage cleaned of old packages when it gets filled. Another weakness of the current implementation is, that ARP requests or not sent again unless new packages to the same unknown address are sent. To solve that a system that would regularly check the IP package storage and send additional ARP requests would have to be implemented.

\section{ICMP - Internet Control Message Protocol}
The ICMP handling was implemented in \verb|usr/drivers/enet/icmp.c| and its interface defined in \verb|usr/drivers/enet/icmp.h|.

The ICMP implementation was straight forward. A handle function was exposed to be called by the IP layer to be called when the protocol type was ICMP. Because we did only have to handle echo request packages, all incoming packages of other types were discarded. Echo requests were answered with echo responses as defined by the ICMP protocol including checksum calculation.

\subsection{Latency}
After finishing the ICMP implementation the performance i.e. latency of the network stack so far could be tested. The following listing \ref{lst:ping5} shows a ping of the Toradex board with 5 requests. We see, that the first request takes a bit longer. This is because the board has to do ARP lookups of the sender first. But we see clearly that all the requests are handled in under 1ms, which we consider to be satisfying.

\begin{lstlisting}[caption=Ping Toradex Board 5 times, label=lst:ping5]
$ ping -c 5 10.0.0.2
PING 10.0.0.2 (10.0.0.2) 56(84) bytes of data.
64 bytes from 10.0.0.2: icmp_seq=1 ttl=64 time=0.567 ms
64 bytes from 10.0.0.2: icmp_seq=2 ttl=64 time=0.242 ms
64 bytes from 10.0.0.2: icmp_seq=3 ttl=64 time=0.244 ms
64 bytes from 10.0.0.2: icmp_seq=4 ttl=64 time=0.192 ms
64 bytes from 10.0.0.2: icmp_seq=5 ttl=64 time=0.223 ms

--- 10.0.0.2 ping statistics ---
5 packets transmitted, 5 received, 0% packet loss, time 4058ms
rtt min/avg/max/mdev = 0.192/0.293/0.567/0.137 ms
\end{lstlisting}

The following listing \ref{lst:ping1000} shows the statistics of sending 1000 ping packages. We can see that no package was lost and the time is consistently at around 0.225ms.

\begin{lstlisting}[caption=Ping Toradex Board 1000 times, label=lst:ping1000]
$ ping -c 1000 10.0.0.2
[...]
1000 packets transmitted, 1000 received, 0% packet loss, time 1012333ms
rtt min/avg/max/mdev = 0.136/0.225/0.367/0.023 ms
\end{lstlisting}

\section{UDP - User Datagram Protocol}
The UDP handling was implemented in \verb|usr/drivers/enet/udp.c| and its interface defined in \verb|usr/drivers/enet/udp.h|.

The UDP implementation was also straight forward. We split the the sending again like we have done previously in the IP and ethernet layer. We completely rely on the IP layer to handle ARP lookups and temporarily storing of IP packages. When sending UDP datagrams we again rely on the client to allocate the storage needed to identify the datagram between the functions, as we did in the IP layer. This again reduces the among of mallocs.

Reserving UDP ports and listening on them by other applications is done in the RPC layer described below. The RPC layer also exposes an interface for other applications to send UDP datagrams.

\subsection{UDP Checksum}
To calculate the UDP checksum we extended the checksum functionality in \verb|lib/netutil/checksum.c| with the function \verb|inet_checksum_IP_pseudo| which also takes the pointer to the start of the IP header. The UDP checksum requires additionally to the UDP header and payload also that a pseudo header containing source IP, destination IP, protocol (which should always be 17 for UDP in this case) and the length of UDP datagram (length of UDP header + length of UDP payload in bytes).
Because of how the IP package and UDP datagram are already written directly to the reserved network buffer to reduce the number of memory copies, the pseudo header is never actually constructed. Instead, a pointer to the IP header is passed to the checksum function along with the pointer to the UDP header and the length of the UDP datagram. Using the IP header, the UDP checksum can be calculated by taking the IP addresses and IP protocol directly from the IP header.
The checksum function was designed to be also usable for calculating the TCP checksum. But this was not tested, as TCP was not implemented in the end due to time restrictions and other functionality having priority.

\section{Interface / API}
The server code for interfacing with other applications was implemented in \verb|usr/drivers/enet/rpc.c| and defined in \verb|usr/drivers/enet/rpc.h| (the RPC layer). The client API was implemented in \verb|lib/aos/netservice.c| and its interface defined in \verb|lib/aos/netservice.h|.

So the netservice is the client, while the RPC layer is the server. Both of them implementing one side of the same RPC protocol. The communication was done over the new nameservice interface, which was implemented in one of the other individual projects. The RPC layer exposes UDP listening and sending using a service called "udp" and printing the ARP cache using a service called "arp".

\subsection{UDP Send}
Sending a UDP datagram is done from any application using the \verb|netservice_udp_send| function defined in netservice. Internally this function does a lookup for the UDP service and then sends a header as defined in the \verb|struct rpc_udp_send| followed by the payload to the service. The service sends back a small response as defined in the \verb|struct rpc_udp_response|.
The response has only feedback in form of a flag that specifies if sending was successful or not. This could be improved upon by sending back result codes for different error cases.

\subsection{UDP Listen}
Listening on a port is a bit more complicated, as the nameservice interface only allows the blocking \verb|nameservice_rpc| function for communication. But because there are potentially indefinitely many UDP messages that have to be received this was not ideal. There were multiple possibilities how to solve the problem:
\begin{enumerate}
  \item Bypass: Use the nameservice to connect the UDP service with a client and then setup the communication between service and client over other means (e.g. over a URPC frame).
  \item Polling: Repeatedly poll from the client on the UPD service for new UDP datagrams.
  \item Anonymous Service: Creating an anonymous service on the client, to which the UDP service can send incoming datagrams for that client.
\end{enumerate}

All of these possibilities have different advantage and disadvantages:
\begin{enumerate}
  \item Bypass: With bypassing we would achieve everything we want, but we would have to implement all of the communication again when there is already an implementation for it. Also we wanted to try to integrate our different individual projects to work together.
  \item Polling: The main disadvantage would be that a lot of internal traffic over the nameservice API would be created. Another problem is that the UDP service would have to manage an additional buffer for incoming UDP datagrams. With the current implementation of nameservice this would also mean an additional copy of the datagram, which we wanted to avoid as a design goal.
  \item Anonymous Service: This approach would have the advantages, that the nameservice API would be used for everything and no datagram buffering in the UDP service would have to be done. This resolves the main disadvantages that the other two options had. One disadvantage with this solution would be, that for each UDP listener a service would be created, which fills the nameserver with unneeded services. Another disadvantages would be, that the setup and tear down would be a lot more complicated, as additional services would have to be registered and hooked up in the UDP service.
\end{enumerate}

We decided to implement the 3rd option (Anonymous Service).
The following listing \ref{lst:udplisten} shows the interface of the API function in netservice:

\begin{lstlisting}[language=c, caption=Interface for UDP listen, label=lst:udplisten]
errval_t netservice_udp_listen(
    uint16_t port, netservice_udp_handler_t udp_handler,
    void *udp_handler_state);
\end{lstlisting}

As we can see, the client only needs to specify a port and a callback closure (consisting of a function pointer and state variable). If the API call succeeds, the callback function would get called with every arriving UDP datagram for the specified port until the UDP port is closed again. (See the next section for closing.)

What happens inside the function is, that the client registers a anonymous service to receive UDP datagrams on and then sends the name of this service along with the port to the UDP service. In the udp service we lookup the given client service name and if this succeeds and the port was still free would add the client nameserver channel to a hashtable. This allows a fast lookup for the correct client channel, when receiving UDP datagrams by searching for the destination port in the hashtable.

When sending UDP datagrams to the client we use a trick that was described earlier in the chapter that makes sure the data is only copied once: When sending a datagram to the client we do not want to send all the header information, but we need to send some auxiliary data (like IP address and port numbers) and some connection information. This data is just added to the back of the buffer after the end of the datagram payload. We are sure that there are enough bytes for that in the buffer, because the buffer has more bytes than needed, because it allocates more bytes than could be possibly received on an ethernet frame.

\subsection{UDP Close}
Because we choose the more difficult setup for UDP listen, closing becomes naturally more difficult as well. There was another important design decision made for closing the UDP ports: Should we allow any application to close just any port or do we want to only allow the client that started listening to be able to close the port. The advantage of the former was, that it is simpler to implement and allows for other applications to reclaim a port, if it was left open by another application. But we see a big disadvantage in being able to close ports of other clients, in that we do not want to allow hijacking of a connection between different applications. Also we want to give the client more stable guarantees, that their connection is not suddenly closed by other applications.

\begin{figure}
  \centering

  \begin{sequencediagram}
    \newthread{cli}{Client}
    \newinst{api}{Netservice API}
    \newinst{clisrv}{Client Service}
    \newthread{srv}{UDP service}

    \begin{call}{cli}{close(port)}{api}{}
      \begin{call}{api}{set\_closing(port)}{api}{}
      \end{call}

      \begin{call}{api}{close(port)}{srv}{}
        \begin{call}{srv}{close\_ok(port)}{clisrv}{okflag}
          \begin{call}{clisrv}{is\_closing(port)}{api}{okflag}
          \end{call}
        \end{call}

        \begin{sdblock}{if}{[okflag]}
          \begin{call}{srv}{close(port)}{srv}{}
          \end{call}
        \end{sdblock}
      \end{call}

    \end{call}
  \end{sequencediagram}

  \caption{Sequence diagram for closing an UDP port}
  \label{fig:udpclose}
\end{figure}

We decided to setup a protocol, that would get the confirmation of the client before closing the port. This confirmation process was completely hidden from the client by the netservice API. The listing \ref{lst:udpclose} shows the API that the client is exposed to. The only thing the client observes is, that if he closes his own connections it works, but if he tries to close foreign connections it is going to fail. The following figure \ref{fig:udpclose} shows how this was achieved.

\begin{lstlisting}[language=c, caption=Interface for UDP close, label=lst:udpclose]
errval_t netservice_udp_close(uint16_t port);
\end{lstlisting}

The client calls the close function in the netservice API. There the local channel state for the given port is searched and the connection is marked as ready to be closed down. Then an RPC call is sent to the UDP service to close the specified port. The UDP service checks if there is a client listening on the port and if so, sends a confirmation request to the client's anonymous service for the given port. The client service can check the closing flag in constant time and answer the UDP service that it is ok to close the port if the flag is set or not ok otherwise.

\subsection{ARP Print Cache}
The ARP service has an own service registered that takes requests to print the ARP cache. Then receiving a request the service simply calls the \verb|arp_print_cache| function which uses printf calls to render the ARP cache to the serial output.
