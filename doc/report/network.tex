\chapter{Network}

\section{Architecture / Design}

\subsection{Running the Network Driver}

One of the earliest decision in the network project was, where the driver should run. One of the considered options was, to start an additional core that would be dedicated to run the network driver and handle the network protocols. The advantages of this approach would be:
\begin{itemize}
    \item Better performance and lower latency: The network could run uninterrupted and be preemted much less and only for the kernel. (This is an assumption, as we did not try to implement this approach and meassure it.)
    \item Fast client applications that use networking: Because the networking would run on a different core than any other application, UMP could be used as the sole message passing system. This would not only make the interface simpler but also faster, as the book has already shown that UMP is faster than LMP.
\end{itemize}
The disadvanteges would be:
\begin{itemize}
    \item Refactoring multi-core memory management: As detailed in previous chapters, we split the memory between the two already running cores. We agreed as a team, that it would not make much sense to split the memory threeways for the network and it is hard to predict which core would need how much memory. So for a networking core to be functional we would have to implement a way to pass memory to the netwroking core, which we previously decided not to do because we deemed it to be a lot of complicated work.
    \item Unrealistic: If we were to continue work on this OS we would at some point want to start all the cores and use them for user applications. At this point we would not want a core dedicated to networking anymore. When looking at the bigger picture, it therefore does not seem to make sense to have a dedicated networking core and would also feel like cheating to be able to show better performance in the final report.
\end{itemize}

Considering the arguments above we decided to run the networking driver in its own domain which is pinned to core 0.

The idea of running the driver in the main process was quickly discarded as we already consider that to be a too big monolith that we would like to trim down. Also the idea of running the driver on multiple cores sounds like a nightmare and was not further considered.

\subsection{Running the Network Protocols}

The next decision was where to run the code for the different network protocols and how they would interact with the driver and each other. The book already suggests adding the protocol specific code to the driver application. This comes at a cost of flexibility and modularity, but is way simpler and faster to implement. additionally the nameservice project was not started at this point, so the communication between protocols would have to be improvised and be rewritten later on.

With those arguments in mind, the decision to implement the protocols inside the driver application. This also allowed to defer the implementation of the communication with other applications until nameservice made some progress, which was in retrospect a great time saver.

\subsection{General Design Decisions}

One of the design goals in the networking project was to reduce copying of data and therefore passing a reference into the same ethernet frame between the network protocol. Another goal was to keep the number of mallocs low and allocate on the stack whenever possible. This should simplify resource management and reduce the amount of bugs.

An important design decison was to keep the interface between the network protocols simple, straight forward and without a lot of abstraction. This makes the network stack less extensible and violates the open-closed principle when e.g. adding a new protocol. The advantages are, that the its faster to implement like this and there is no overhead caused from the abstraction.

That beeing said, we made it a goal to separate the code for the different protocols: Each network protocol has its own files and a clearly defined interface for interacting with other protocols. For example the IP protocol has its own header and source file and all the IP code is inside those files. When calling functions to send IP packages, the code of the IP protocol will call functions of the ethernet implementation to send IP packages over ethernet frames.

\section{Driver}

The driver to interact with the networking hardware was given in the handout. Because its functionality was not changed, the driver itself will not be covered in the report but only the interaction with it.

At startup the driver is handed two large frames: One to write incoming ethernet frames to and the other to read outgoing ethernet frames from. The driver uses slices that are half a page each per ethernet frame. So that a page is used for up to two ethernet frames.

To interact with the driver enqueue and dequeue operations are used. To get access to received ethernet frames the dequeue operation is used. It gives an offset into one of the memory regions used and blocks that part of the memory until it is released back to the driver. To do so the enqueue operation is used. Transmiting outgoing ethernet frames works in a similar way. The client of the driver has to keep track on which frames can be written to. Ethernet frames can be handed to the driver with an enqueue operation. To get back access to the memory after the sending is done, the dequeue operation has to be called again.

Because the enqueue and dequeue operations do not give a virtual address or pointer to work on, but only an offset to the base of the large frames passed to it, we decided to map those frames again a second time for use in the protocols. This we operate on different virtual addresses for the same physical memory in the same domain. The advantage of this was, that the queue interface used by the driver had not to be changed or rewritten.

\subsection{Disabled Caching}
A big problem early on was, that parts of the sent ethernet frames were sometimes not correct. On closer investigation the wrong parts were identical to earlier ethernet frames (e.g. sending an ICMP echo reply contained bytes from an earlier sent ARP request). After suspecting a caching problem, the memory used to interact with the driver was mapped with deactivated caches and this solved all of these problems. Later on we discovered that the same was suggested on the moodle forum to other groups, so we did not search for an alternative. But we suspect that this is currently the main bottleneck in the networking code.

\subsection{Copy of Data}
Earlier we said that a design goal was to keep the copying of data to a minimum. But after seeing how the ethernet driver worked we decided to not share to which the driver wrote. The main reason for this was a security concern: The minimal amount of memory that can be shared is one page. But one page holds two ethernet frames. Because two ethernet frames sharing a page might not belong to the same client it would be unsafe to share the memory, as then a client could get access to data that belongs to another client. This could even be forced by producing a lot of traffic.

It would be maybe possible to rewrite the driver to handle it differently, so that the memory could safely be shared with clients. But we deemed this to be to big of a task for the limited time available.

So we had to add at least one copy to send the data to the client. But we made sure it was minimal, by only copying the payload, but none of the protocol headers. Beacause we wanted to send some additional data like the ip address and port numbers, those were copied to the back of the memory, right after the payload. This can be safely done, because the memory reserved per ethernet frame by the driver is substentally bigger than an ethernet frame could be. The driver does this, so the ethernet frames are aligned to half the page size.

\section{Ethernet}
The ethernet handling was implemented in \verb|usr/drivers/enet/ethernet.c| and its interface defined in \verb|usr/drivers/enet/ethernet.h|.

\subsection{Receiving Frames}
Receiving frames was rather straight forward. The main event loop in \verb|usr/drivers/enet/enet_module.c| is constantly checking for incoming ethernet frames. When receiving an ethernet frame (over driver dequeue), it calls the ethernet handle function which is blocking. As soon as this function returns the memory slice in which the incoming ethernet frames resided in was released again by calling enqueue on the driver.

The handle function would look at the ethernet frame and call the IP or ARP handle functions respectively. The ethernet frame would be discarded and not handled if the protocol was unknown or the destination ethernet address was invalid. The address would be considered invalid if it was not a broadcast address or the ethernet address of the Toradex board.

\subsection{Sending Frames}
\label{sec:sendframe}
The main difficulty for this part was managing the memory regions for sending ethernet frames correctly and efficiently. We decided to create a linked list with nodes for each ethernet frame sized memory slice. If the node was in the linked list, the slice was not currently in use. Because we already know the amount of slices in advance the nodes could be allocated all at once and the linked list created when initialising ethernet.

What makes this approach performant is, that the list has never to be traversed but only ever the first element has to be touched in each operation: For reserving a slice that can later on be sent, the front element of the list has to be removed and its address remembered. To release the reserved slice again the remembered element had to be simply added back to the front of the list.

\begin{figure}
    \centering

    \begin{sequencediagram}
      \newthread{cli}{client}
      \newinst{eth}{ethernet}
      \newinst{drv}{driver}
  
      \begin{call}{cli}{start\_send()}{eth}{id, ptr}
        \begin{sdblock}{loop}{}
            \begin{call}{eth}{dequeue()}{drv}{offset}
            \end{call}
        \end{sdblock}
      \end{call}

      \begin{call}{cli}{write\_payload(ptr)}{cli}{}
      \end{call}

      \begin{call}{cli}{send(id)}{eth}{}
        \begin{call}{eth}{enqueue()}{drv}{}
        \end{call}
      \end{call}
    \end{sequencediagram}

    \caption{Sequence diagram for sending ethernet frames}
    \label{fig:ethsend}
\end{figure}

The interface to send ethernet frames was built on that approach. Figure \ref{fig:ethsend} shows a sequence diagram of said interface. It consits of a function to start sending an ethernet frame, which returns an id and a pointer. The pointer can then be used to write the payload of the ethernet frame (e.g. an ip package). When done writing, the client calls the send function with the id. This id can directly be used by the ethernet protocol to enqueue the correct memory slice to the driver.

Another important part is, that the memory is not immediately free to be used again, because the data has to be sent by the hardware before that. This is done in parallel and it would be a waste of performance to block on that. So we decided to keep the data reserved and return from the send function. How data is released again can be seen in the same sequence diagram \ref{fig:ethsend}. In the start send function, ethernet dequeues all slices that were done sending from the driver. To figure out which list nodes to add in the tracking of free memory slices we use some pointer arithmetic. By using the fact that all of the list nodes were allocated as one big chunk and the address of each list node therefore correspondes to the address of a memory slice, the address of the correct list node can be quickly calculated using the offset returned by dequeue.

\section{ARP - Address Resolution Protocol}
The ARP handling was implemented in \verb|usr/drivers/enet/arp.c| and its interface defined in \verb|usr/drivers/enet/arp.h|.

\subsection{ARP Cache}
\label{sec:arpcache}
The ARP cache was saved using the provided hashtable in \verb|lib/collections|. The reasoning behind this was, that we need a datastructure that can do the lookup of an address as fast as possible but does not have to be as fast to insert or remove entries. Hashtables fulfill exactly these criteria. Only the IP address to ethernet address lookup had to be implemented, so a single hashtable was sufficient.

Another decision was in which byte order to store the addresses. We decided to store the ethernet addresses in network byte order. The main advantage of this is, that in the majority uf uses, the ethernet address was needed in network byte order anyways, and so the conversion did not have to be done for every IP package over and over again. Additionally ARP receives the ethernet addresses in network byte order anyways so conversion could be saved here too. The only place were the byte order conversion had to be done was in the cache printing.

\subsection{Interfaces}
The ARP layer is strongly coupled with the ethernet layer, as it uses the ethernet sending functions to send ARP responses and requests and ethernet calls the ARP handling function if it received packages of the ARP protocol type.

The ARP layer is additionally strongly coupled with the IP layer: It offers a function that allows sending ARP requests to the given IP address and it notifies the IP layer when a new entry for an IP address is added to the ARP cache. This notification is done, so ip packages that are waiting for an address resolution can be sent (more on that in the IP part below).

\subsection{Printing ARP Cache}
Another interface that the ARP layer offers is printing the ARP cache. We decided to not send a copy of the ARP cache to the shell and do the printing there, but to directly use printf functions inside the ARP layer to render the cache for the serial output. This was mainly done, because it seemed to be a simpler way of doing it in terms of how much effort it would take to implement. Sharing the memory of the ARP cache was not considered, because that sounds like a security nightmare. As mentioned before in \ref{sec:arpcache}, when printing we had to also convert from network byte order.

\subsection{Additional Features}
ARP was implemented to support the additional features of sendin ARP probe packages and handling ARP anouncements.

An ARP probe package is a request send out to the network to check if anyone uses the IP address that we would like to start using. After starting the ARP protocol an ARP probe is sent to the network, checking if our static IP (hardcoded 10.0.0.2) is already in use. If ARP ever gets an external entry for that static IP address, it stops the network driver. In the future this could be expanded to then request another IP address from DHCP or from the user.

ARP anouncements are ARP request sent on ethernet broadcast addresses to tell other network members, that they are using the anounced IP address. By handling these anouncements, the ARP cache can be populated without having to send ARP requests to these targets. But more importantly handling them again reduces the risk of an IP address conflict.

\section{IP - Internet Protocol}
The IP handling was implemented in \verb|usr/drivers/enet/ip.c| and its interface defined in \verb|usr/drivers/enet/ip.h|.

\subsection{Receiving Packages}
Receiving IP packages is tightly coupled with the ehternet, ICMP and UDP layers. The ethernet layer calls the IP layer handle function when receiving an ethernet frame with the protocol bytes set to IP. The IP layer then looks at the package and sends it on to the ICMP or UDP layers depending on the specified protocol.

The package is discarded in the IP layer if...
\begin{itemize}
    \item ...the version of the package was not 4 (for IPv4).
    \item ...the package was part of a fragmented package. (We did not implement handling fragmented packages.)
    \item ...the package was not sent to our static IP address and also not to one of the broadcast addresses.
    \item ...the package was for a protocol other than ICMP or UDP.
\end{itemize}

The checksum of the IP package is checked and a message is printed if the checksum was incorrect but the package was not discarded in that case. It would be easy to also discard a package in that case, but we decided against doing so for now, because we did not want to discard packages if we misscalculated the checksum on our end (which happened multiple times in early development).

\subsection{Sending Packages}
Sending IP packages is tightly coupled with the ethernet, ICMP and UDP layers too. Because of how the interface of ethernet has split the sending functions in two (see \ref{sec:sendframe}), we decided to make this transparent to the client and implement IP package sending in the same style.

So there are again two functions, one for start sending a package and one for actually sending it. The start sending function returns a pointer to which the package payload can be written. It is noteworthy that to reduce the amount of mallocs (which is one of the design goals) we make the client allocate the storage needed to identify the package between the two sending functions. This means that the client can allocate that on the stack.

The start sending function takes (beside others) a parameter with the target IP address. Before it can call start send ethernet frame it needs to know the target ethernet address. For that the ARP lookup function is called to lookup the ethernet address given the target IP address.

\begin{figure}
    \centering

    \begin{sequencediagram}
      \newthread{cli}{client}
      \newinst{ip}{IP}
      \newinst{arp}{ARP}
      \newthread{eth}{ethernet}
  
      \begin{call}{cli}{start\_send(ip)}{ip}{id, ptr}
        \begin{call}{ip}{lookup\_ip(ip)}{arp}{not\_found}
          \begin{call}{arp}{send\_arp\_req(ip)}{eth}{sent}
          \end{call}
        \end{call}

        \begin{call}{ip}{alloc\_store(ip, id)}{ip}{ptr}
        \end{call}
      \end{call}

      \begin{call}{cli}{write\_payload(ptr)}{cli}{}
      \end{call}

      \begin{call}{cli}{send(id)}{ip}{}
        \begin{call}{ip}{store(id)}{ip}{}
        \end{call}
      \end{call}

      \begin{call}{eth}{arp\_rep(ip, eth)}{arp}{}
        \begin{call}{arp}{notify(ip, eth)}{ip}{}
          \begin{sdblock}{loop}{}
            \begin{call}{ip}{send\_ip()}{eth}{sent}
            \end{call}
          \end{sdblock}
        \end{call}
      \end{call}
    \end{sequencediagram}

    \caption{Sequence diagram for storing IP packages until ARP concludes}
    \label{fig:ipsend}
\end{figure}

In case looking up the ethernet address failed, we did not just want to discard the package. So the following was decided: We store IP packages up to a maximal amount in memory until we learn their ethernet address. Figure \ref{fig:ipsend} shows how this works. After looking up the IP address and finding that we do not know the corresponding ethernet address we send an ARP request for the given IP address and use malloc to create a temporary storage for the IP package. The client can then write the payload without knowing, that the package can not be immediately sent. For the client both cases are the same.

When the ARP adds an entry for an IP address it notifies the IP layer. There all packages that are stored for the given IP address are sent to the given ethernet address and the local buffer gets freed up.

This system has still room for improvement: We never get rid of stored IP packages to a target that is not reachable and can therefore not respond to an ARP request. So to solve that we could for example stored and the IP package storage cleaned of old packages when it gets filled. Another weakness of the current implementation is, that ARP requests or not sent again unless new packages to the same unknown address are sent. To solve that a system that would regularly check the IP package storage and send additional ARP requests would have to be implemented.

\section{ICMP - Internet Control Message Protocol}

\section{UDP - User Datagram Protocol}

\subsection{UDP Checksum}
To calculate the UDP checksum we extended the checksum functionality in \verb|lib/netutil/checksum.c| with the function \verb|inet_checksum_IP_pseudo| which also takes the pointer to the start of the IP header. The UDP checksum requires additionally to the UDP header and payload also that a pseudo header containing source IP, destination IP, protocol (which should always be 17 for UDP in this case) and the length of UDP datagram (length of UDP header + length of UDP payload in bytes).
Because of how the IP package and UDP datagram are already written directly to the reserved network buffer to reduce the number of memory copies, the pseudo header is never actually constructed. Instead, a pointer to the IP header is passed to the checksum function along with the pointer to the UDP header and the length of the UDP datagram. Using the IP header, the UDP checksum can be calculated by taking the IP addresses and IP protocol directly from the IP header.
The checksum function was designed to be also useable for calculating the TCP checksum. But this was not tested, as TCP was not implemented in the end due to time restrictions and other functionality having priority.

\section{Interface / API}

\section{Nameservice Interaction}