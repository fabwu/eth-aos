\chapter{Network}

\section{Architecture / Design}

\subsection{Running the Network Driver}

One of the earliest decision in the network project was, where the driver should run. One of the considered options was, to start an additional core that would be dedicated to run the network driver and handle the network protocols. The advantages of this approach would be:
\begin{itemize}
    \item Better performance and lower latency: The network could run uninterrupted and be preemted much less and only for the kernel. (This is an assumption, as we did not try to implement this approach and meassure it.)
    \item Fast client applications that use networking: Because the networking would run on a different core than any other application, UMP could be used as the sole message passing system. This would not only make the interface simpler but also faster, as the book has already shown that UMP is faster than LMP.
\end{itemize}
The disadvanteges would be:
\begin{itemize}
    \item Refactoring multi-core memory management: As detailed in previous chapters, we split the memory between the two already running cores. We agreed as a team, that it would not make much sense to split the memory threeways for the network and it is hard to predict which core would need how much memory. So for a networking core to be functional we would have to implement a way to pass memory to the netwroking core, which we previously decided not to do because we deemed it to be a lot of complicated work.
    \item Unrealistic: If we were to continue work on this OS we would at some point want to start all the cores and use them for user applications. At this point we would not want a core dedicated to networking anymore. When looking at the bigger picture, it therefore does not seem to make sense to have a dedicated networking core and would also feel like cheating to be able to show better performance in the final report.
\end{itemize}

Considering the arguments above we decided to run the networking driver in its own domain which is pinned to core 0.

The idea of running the driver in the main process was quickly discarded as we already consider that to be a too big monolith that we would like to trim down. Also the idea of running the driver on multiple cores sounds like a nightmare and was not further considered.

\subsection{Running the Network Protocols}

The next decision was where to run the code for the different network protocols and how they would interact with the driver and each other. The book already suggests adding the protocol specific code to the driver application. This comes at a cost of flexibility and modularity, but is way simpler and faster to implement. additionally the nameservice project was not started at this point, so the communication between protocols would have to be improvised and be rewritten later on.

With those arguments in mind, the decision to implement the protocols inside the driver application. This also allowed to defer the implementation of the communication with other applications until nameservice made some progress, which was in retrospect a great time saver.

\subsection{General Design Decisions}

One of the design goals in the networking project was to reduce copying of data and therefore passing a reference into the same ethernet frame between the network protocol. Another goal was to keep the number of mallocs low and allocate on the stack whenever possible. This should simplify resource management and reduce the amount of bugs.

An important design decison was to keep the interface between the network protocols simple, straight forward and without a lot of abstraction. This makes the network stack less extensible and violates the open-closed principle when e.g. adding a new protocol. The advantages are, that the its faster to implement like this and there is no overhead caused from the abstraction.

That beeing said, we made it a goal to separate the code for the different protocols: Each network protocol has its own files and a clearly defined interface for interacting with other protocols. For example the IP protocol has its own header and source file and all the IP code is inside those files. When calling functions to send IP packages, the code of the IP protocol will call functions of the ethernet implementation to send IP packages over ethernet frames.

\section{Driver}

The driver to interact with the networking hardware was given in the handout. Because its functionality was not changed, the driver itself will not be covered in the report but only the interaction with it.

At startup the driver is handed two large frames: One to write incoming ethernet frames to and the other to read outgoing ethernet frames from. The driver uses slices that are half a page each per ethernet frame. So that a page is used for up to two ethernet frames.

To interact with the driver enqueue and dequeue operations are used. To get access to received ethernet frames the dequeue operation is used. It gives an offset into one of the memory regions used and blocks that part of the memory until it is released back to the driver. To do so the enqueue operation is used. Transmiting outgoing ethernet frames works in a similar way. The client of the driver has to keep track on which frames can be written to. Ethernet frames can be handed to the driver with an enqueue operation. To get back access to the memory after the sending is done, the dequeue operation has to be called again.

Because the enqueue and dequeue operations do not give a virtual address or pointer to work on, but only an offset to the base of the large frames passed to it, we decided to map those frames again a second time for use in the protocols. This we operate on different virtual addresses for the same physical memory in the same domain. The advantage of this was, that the queue interface used by the driver had not to be changed or rewritten.

\subsection{Disabled Caching}
A big problem early on was, that parts of the sent ethernet frames were sometimes not correct. On closer investigation the wrong parts were identical to earlier ethernet frames (e.g. sending an ICMP echo reply contained bytes from an earlier sent ARP request). After suspecting a caching problem, the memory used to interact with the driver was mapped with deactivated caches and this solved all of these problems. Later on we discovered that the same was suggested on the moodle forum to other groups, so we did not search for an alternative. But we suspect that this is currently the main bottleneck in the networking code.

\subsection{Copy of Data}
Earlier we said that a design goal was to keep the copying of data to a minimum. But after seeing how the ethernet driver worked we decided to not share to which the driver wrote. The main reason for this was a security concern: The minimal amount of memory that can be shared is one page. But one page holds two ethernet frames. Because two ethernet frames sharing a page might not belong to the same client it would be unsafe to share the memory, as then a client could get access to data that belongs to another client. This could even be forced by producing a lot of traffic.

It would be maybe possible to rewrite the driver to handle it differently, so that the memory could safely be shared with clients. But we deemed this to be to big of a task for the limited time available.

So we had to add at least one copy to send the data to the client. But we made sure it was minimal, by only copying the payload, but none of the protocol headers. Beacause we wanted to send some additional data like the ip address and port numbers, those were copied to the back of the memory, right after the payload. This can be safely done, because the memory reserved per ethernet frame by the driver is substentally bigger than an ethernet frame could be. The driver does this, so the ethernet frames are aligned to half the page size.

\section{Ethernet}
The ethernet handling was implemented in \verb|usr/drivers/enet/ethernet.c| and its interface defined in \verb|usr/drivers/enet/ethernet.h|.

\subsection{Receiving Frames}
Receiving frames was rather straight forward. The main event loop in \verb|usr/drivers/enet/enet_module.c| is constantly checking for incoming ethernet frames. When receiving an ethernet frame (over driver dequeue), it calls the ethernet handle function which is blocking. As soon as this function returns the memory slice in which the incoming ethernet frames resided in was released again by calling enqueue on the driver.

The handle function would look at the ethernet frame and call the IP or ARP handle functions respectively. The ethernet frame would be discarded and not handled if the protocol was unknown or the destination ethernet address was invalid. The address would be considered invalid if it was not a broadcast address or the ethernet address of the Toradex board.

\subsection{Sending Frames}
The main difficulty for this part was managing the memory regions for sending ethernet frames correctly and efficiently. We decided to create a linked list with nodes for each ethernet frame sized memory slice. If the node was in the linked list, the slice was not currently in use. Because we already know the amount of slices in advance the nodes could be allocated all at once and the linked list created when initialising ethernet.

What makes this approach performant is, that the list has never to be traversed but only ever the first element has to be touched in each operation: For reserving a slice that can later on be sent, the front element of the list has to be removed and its address remembered. To release the reserved slice again the remembered element had to be simply added back to the front of the list.

\begin{figure}
    \centering

    \begin{sequencediagram}
      \newthread{cli}{client}
      \newinst{eth}{ethernet}
      \newinst{drv}{driver}
  
      \begin{call}{cli}{start\_send()}{eth}{id, ptr}
        \begin{sdblock}{loop}{}
            \begin{call}{eth}{dequeue}{drv}{}
            \end{call}
        \end{sdblock}
      \end{call}

      \begin{call}{cli}{write\_payload(ptr)}{cli}{}
      \end{call}

      \begin{call}{cli}{send(id)}{eth}{}
        \begin{call}{eth}{enqueue()}{drv}{}
        \end{call}
      \end{call}
    \end{sequencediagram}

    \caption{Sequence diagram for sending ethernet frames}
    \label{fig:ethsend}
\end{figure}


The interface to send ethernet frames was built on that approach. Figure \ref{fig:ethsend} shows a sequence diagram of said interface. It consits of a function to start sending an ethernet frame, which returns an id and a pointer. The pointer can then be used to write the payload of the ethernet frame (e.g. an ip package). When done writing, the client calls the send function with the id. This id can directly be used by the ethernet protocol to enqueue the correct memory slice to the driver.

Another important part is, that the memory is not immediately free to be used again, because the data has to be sent by the hardware before that. This is done in parallel and it would be a waste of performance to block on that. So we decided to keep the data reserved and return from the send function. How data is released again can be seen in the same sequence diagram \ref{fig:ethsend}. In the start send function, ethernet dequeues all slices that were done sending from the driver. To figure out which list nodes to add in the tracking of free memory slices we use some pointer arithmetic. By using the fact that all of the list nodes were allocated as one big chunk and the address of each list node therefore correspondes to the address of a memory slice, the address of the correct list node can be quickly calculated using the offset returned by dequeue.

\section{ARP - Address Resolution Protocol}
The ARP handling was implemented in \verb|usr/drivers/enet/arp.c| and its interface defined in \verb|usr/drivers/enet/arp.h|.

\subsection{ARP Cache}
\label{sec:arpcache}
The ARP cache was saved using the provided hashtable in \verb|lib/collections|. The reasoning behind this was, that we need a datastructure that can do the lookup of an address as fast as possible but does not have to be as fast to insert or remove entries. Hashtables fulfill exactly these criteria. Only the IP address to ethernet address lookup had to be implemented, so a single hashtable was sufficient.

Another decision was in which byte order to store the addresses. We decided to store the ethernet addresses in network byte order. The main advantage of this is, that in the majority uf uses, the ethernet address was needed in network byte order anyways, and so the conversion did not have to be done for every IP package over and over again. Additionally ARP receives the ethernet addresses in network byte order anyways so conversion could be saved here too. The only place were the byte order conversion had to be done was in the cache printing.

\subsection{Interfaces}
The ARP layer is strongly coupled with the ethernet layer, as it uses the ethernet sending functions to send ARP responses and requests and ethernet calls the ARP handling function if it received packages of the ARP protocol type.

The ARP layer is additionally strongly coupled with the IP layer: It offers a function that allows sending ARP requests to the given IP address and it notifies the IP layer when a new entry for an IP address is added to the ARP cache. This notification is done, so ip packages that are waiting for an address resolution can be sent (more on that in the IP part below).

\subsection{Printing ARP Cache}
Another interface that the ARP layer offers is printing the ARP cache. We decided to not send a copy of the ARP cache to the shell and do the printing there, but to directly use printf functions inside the ARP layer to render the cache for the serial output. This was mainly done, because it seemed to be a simpler way of doing it in terms of how much effort it would take to implement. Sharing the memory of the ARP cache was not considered, because that sounds like a security nightmare. As mentioned before in \ref{sec:arpcache}, when printing we had to also convert from network byte order.

\subsection{Additional Features}
ARP was implemented to support the additional features of sendin ARP probe packages and handling ARP anouncements.

An ARP probe package is a request send out to the network to check if anyone uses the IP address that we would like to start using. After starting the ARP protocol an ARP probe is sent to the network, checking if our static IP (hardcoded 10.0.0.2) is already in use. If ARP ever gets an external entry for that static IP address, it stops the network driver. In the future this could be expanded to then request another IP address from DHCP or from the user.

ARP anouncements are ARP request sent on ethernet broadcast addresses to tell other network members, that they are using the anounced IP address. By handling these anouncements, the ARP cache can be populated without having to send ARP requests to these targets. But more importantly handling them again reduces the risk of an IP address conflict.


\section{IP - Internet Protocol}

\section{ICMP - Internet Control Message Protocol}

\section{UDP - User Datagram Protocol}

\subsection{UDP Checksum}
To calculate the UDP checksum we extended the checksum functionality in \verb|lib/netutil/checksum.c| with the function \verb|inet_checksum_IP_pseudo| which also takes the pointer to the start of the IP header. The UDP checksum requires additionally to the UDP header and payload also that a pseudo header containing source IP, destination IP, protocol (which should always be 17 for UDP in this case) and the length of UDP datagram (length of UDP header + length of UDP payload in bytes).
Because of how the IP package and UDP datagram are already written directly to the reserved network buffer to reduce the number of memory copies, the pseudo header is never actually constructed. Instead, a pointer to the IP header is passed to the checksum function along with the pointer to the UDP header and the length of the UDP datagram. Using the IP header, the UDP checksum can be calculated by taking the IP addresses and IP protocol directly from the IP header.
The checksum function was designed to be also useable for calculating the TCP checksum. But this was not tested, as TCP was not implemented in the end due to time restrictions and other functionality having priority.

\section{Interface / API}

\section{Nameservice Interaction}