\chapter{Milestone 3 - (Lightweight) Message Passing}

Documented here is how we designed and implemented the LMP (Lightweight Message Passing) during milestone 3 and some of the improvements we made later on. The changes made in the individual project nameserver are documented in their own chapter. The nameserver project made an overhaul on the LMP and the final code base might look substentially different than what is documented here.

\section{Architecture / Design}

\subsection{Init Monolith vs. Many Services}

One of the first big design decisions in this milestone was, where to run the different services. There were the terminal service and memory service that had to be implemented during this milestone and we knew that later on we had to have a process management service. We considered two options: Running all of those services in the init domain or separating them and starting an own domain for each service.

Disadvantages for a monolithic init:
\begin{itemize}
    \item Complexity: Having all the services in init instead of separating them means more complexity. The interfaces between different services are not that well defined and it is harder to maintain and test a monolith.
\end{itemize}

Advanteges for a monolithic init:
\begin{itemize}
    \item Performance: Having all the services run in the same domain and on the same thread means less context switches and when staying in init no context switches at all.
    \item Easier to develop: Adding the services to init meant we did not have to rewrite any of the working functionality inside of init but could just provide an interface for other applications.
\end{itemize}

We decided to implement the monolithic init. We did so mainly because we considered it way easier and faster to implement. Moving for example the memory service out of init meant that we would request memory from that service in init and we were at that point not confident that we could handle all the entailing problems in time for the milestone submission.

\section{Channel Setup - Child and Init}

One of the first tasks we had to handle was setting up the communication between init and child domains after spawning them. The following explains how we handled this during milestone 3.

We passed the init endpoint at a well known location in the childs task cnode when spawning the child. Afterwards we setup the communication between child and init by passing the childs endpoint to init and then sending a confirmation message from init to the child. The child only starts running from its main function after receiving this confirmation from init. The following describes these steps in more detail with some implementation details:

\begin{enumerate}
    \item When spawning a new child: Pass init endpoint in task cnode in slot defined by constant \verb|TASKCN_SLOT_INITEP| using \verb|cap_copy|. This is done in the function \verb|spawn_child_cspace_set_initep| in \verb|lib/spawn/spawn.c|.
    \item In \verb|create_child_channel| (spawn.c) register
    \verb|recv_setup_closure| from init
    (used to receive initial ep from child)
    \item in init.c (called before each thread):
        \begin{itemize}
            \item create child endpoint i.e channel for this child (we are already in child)
            \item register \verb|receive_init_closure| (used to save ep received from init)
            \item send child ep to init and wait until
            \verb|receive_init_closure| get called
        \end{itemize}
    \item \verb|recv_setup_closure| gets called:
        \begin{itemize} 
            \item save child ep in init channel struct (\verb|lmp_chan|)
            \item send message to child that channel is ready (in \verb|rpc_send_setup_closure|)
        \end{itemize}
    \item \verb|barrelfish_recv_init_closure| gets called on the child side
        \begin{itemize}
            \item child is now ready to communicate with init
            \item on child side continue after recv init success
            \item init is now dispatching events in \verb|recv_regular_closure|
        \end{itemize}
\end{enumerate}

\section{RPC Implementation}

In this section we detail how RPC over LMP was initially implemented. After setting up the communication between init and its children, mainly two files are concerned when talking about RPC at this stage of the project: \verb|lib/aos/aos_rpc.c| contains the implementation of all RPC functions that can be called by the child to communicate with a service. \verb|usr/init/rpc.c| contains the implementation of the services and their RPC interactions in init.

\subsection{Message Type}
When sending a message over LMP we can send 4 * 64 bytes of data and a capability. In our RPC implementation we reserve the first 64 bytes of each call for metadata. At this stage of the development we store the message type in the lowest 8 bytes of those 64 bytes. The message type is used to determine in init, which RPC call was made and in the child to check if the received response was for the correct RPC call.

In \verb|usr/init/rpc.c| there's a switch statement that is used to route the incoming RPC calls to the correct service function using this message type. The rest of the bytes are then interpreted by the service functions.

\subsection{init - Service Handler}
The init process was at this point dispatching waitset events at the end of the main function in \verb|usr/init/main.c|. After a communication with a child was setup, each time a child sent something to init over LMP it was received in \verb|usr/init/rpc.c| in the function \verb|rpc_handler_recv_closure|. That is also the where the aforementioned switch for the message type is located.
After selecting the correct service and handling the RPC request, a receive closure with the function \verb|rpc_handler_recv_closure| was registered again for the child channel. The service functions were designed to be non-blocking and could therefore only receive a single LMP message per call. This turned out to be too limiting as we wanted to receive more than that in some service functions.

\subsection{child - aos\_rpc}
The \verb|aos/aos_rpc.h| interface has a set of blocking functions that can be called by any child. Each child sets up the internal state of aos\_rpc and the channel communication with init before its main function gets called. There are multiple functions that return the RPC channel to the different services. All of those returned the init channel, as init handled all the RPC calls at this point.
In the main function a child could call any RPC function, which blocked and reported success or failure over the \verb|errval_t| return value. For example to send a character to the terminal service (running in init) the client could do the following:

\begin{lstlisting}[language=c, caption=Example of child calling an RPC function]
struct aos_rpc *serial = aos_rpc_get_serial_channel();
errval_t err = aos_rpc_serial_putchar(serial, 'c');
bool success = err_is_ok(err);
\end{lstlisting}

In the listing above the client gets the serial RPC channel (which in our case is pointing to init) and then sends the character 'c' over the blocking call \verb|aos_rpc_serial_putchar| and is able to tell if the transfer was successful by checking the returned \verb|errval_t| value.

\subsection{Blocking RPC}
Making the RPC calls blocking was not trivial and was deliberated quite a bit before comming to a conclusion on how we wanted to implement it. We knew that it was required to make the RPC calls blocking, but we had to still be able to dispatch waitset events, so that LMP reponses could actually be received while blocking. So we implemented a function \verb|aos_rpc_dispatch_until_set| in \verb|lib/aos/aos_rpc.c| which would take a pointer to a flag and then continuously dispatch on the waitset until this flag was set to true. Before calling this function we would register a closure to receive LMP messages from init on and set the bool flag to true when a message from init was received allowing \verb|aos_rpc_dispatch_until_set| to return.

\section{Realisations and Improvements}

When revising our work after finishing milestone 3 we realised that we had a lot of code duplication in the LMP channel handling. Having multiple functions of setting up a closure, registering receive closures, re-registering closures upon transient failure and so on only to set a flag seemed overcomplicated. So we wanted something to make it easier to write LMP code by abstracting away from the provided bare LMP channel functionality.

We also noticed that we experienced a lot of stack ripping as it was described in the book. Following the logic of the communication was made harder as for each message that was sent or received a closure has to be built and registered. The logic of a single RPC call was in init spread over several functions and hard to track.

In the child (in \verb|lib/aos/aos_rpc.c|) some effort was already made in that direction. There the RPC functions had to be blocking, which already required some abstraction. By providing the functions \verb|aos_rpc_lmp_send| and \verb|aos_rpc_lmp_call| as an interface for the RPC functions the LMP channel plumbing could be somewhat abstracted away.

Based on this idea a general LMP interface was developed, which hides all the underlying work with closure and provides a simple blocking interface which was then used by the child (in \verb|lib/aos/aos_rpc.c|) and in init (\verb|usr/init/rpc.c|). The following section describes this new abstraction layer in more detail:

\subsection{LMP Protocol}

The abstraction was defined in the header (\verb|aos/lmp_protocol.h|) and implemented in the source file (\verb|lib/aos/lmp_protocol.c|). There functions for sending and receiving LMP messages were provided which had an interface like the following:
\begin{itemize}
    \item Simple send
    \begin{verbatim}errval_t lmp_protocol_send(struct lmp_chan *chan,
    uint16_t message_type, struct capref cap,
    uintptr_t arg1, uintptr_t arg2, uintptr_t arg3)\end{verbatim}
    \item Simple receive
    \begin{verbatim}errval_t lmp_protocol_recv(struct lmp_chan *chan,
    uint16_t message_type, struct capref *ret_cap,
    uintptr_t *ret_arg1, uintptr_t *ret_arg2, uintptr_t *ret_arg3)\end{verbatim}
    \item Send bytes
    \begin{verbatim}errval_t lmp_protocol_send_bytes_cap(
    struct lmp_chan *chan, uint16_t message_type, struct capref cap,
    size_t size, const uint8_t *bytes)\end{verbatim}
    \item Receive bytes
    \begin{verbatim}errval_t lmp_protocol_recv_bytes_cap(struct lmp_chan *chan,
    uint16_t message_type, struct capref *ret_cap,
    size_t *ret_size, uint8_t **ret_bytes)\end{verbatim}
\end{itemize}

All capabilities (cap) and numerical arguments (arg1, arg2, arg3) could be omitted and so further definitions were included to provide an interface for all possible combinations of arguments and capabilities. All these send and receive function in \verb|lmp_protocol| were blocking. Always waiting for the registered closure to be handled before returning. This blocking mechanism was implemented the same way as described above for "blocking RPC". The following snippet shows how this interface was used in the \verb|aos_rpc_process_spawn| function (code is simplified for demonstration):

\begin{lstlisting}[language=c, caption=Usage of aos/lmp\_protocol in aos\_rpc\_process\_spawn]
// Request process spawn
lmp_protocol_send0(&rpc->chan, AOS_RPC_PROCESS_SPAWN);
// Send commandline
lmp_protocol_send_string(&rpc->chan,
    AOS_RPC_PROCESS_SPAWN_CMD, cmdline);
// Get pid and success information
lmp_protocol_recv2(&rpc->chan,
    AOS_RPC_PROCESS_SPAWN, &ret_pid, &ret_success);
\end{lstlisting}

In the listing above we see a more complex communication than was previously implemented. We see that process spawning is initiated by sending a spawn request that consits only of the message type and 0 additional bytes. That is also what the 0 in \verb|lmp_protocol_send0| stands for. This is followed by sending a string containing the command line which should be used to spawn the process. At the end 2 integers are received which contain a success flag and if the spawning was successful the pid of the newly spawned process.

The code on the init side was adjusted accordingly to handle RPC calls with multiple LMP messages per call. The main entry point in \verb|usr/init/rpc.c| (function \verb|rpc_handler_recv_closure|) was left unchanged, but the service functions are now blocking functions, that also use the \verb|aos/lmp_protocol.h| interface.
For example the following listing shows an extract of the spawn process service function:

\begin{lstlisting}[language=c, caption=Usage of aos/lmp\_protocol in usr/init/rpc.c]
// Receive and parse commandline
lmp_protocol_recv_string(chan,
    AOS_RPC_PROCESS_SPAWN_CMD, &cmdline);
char **argv = make_argv(cmdline, &argc, &buf);
// Spawn process
init_spawn(argv[0], &pid);
// Send back pid and success flag
lmp_protocol_send2(chan,
    AOS_RPC_PROCESS_SPAWN, pid, true);
\end{lstlisting}

The listing above was again substentially shortened. Leaving out all of the error handling and the declarations. But it should show, that it is a close parallel to the \verb|lmp_protocol| calls made on the client side, just swaping out receive and send in the respective functions.
Noteworthy is, that there is no call to receive the first empty LMP message on the service side. This is because the first LMP message is always received by \verb|rpc_handler_recv_closure|. To prevent that all RPC calls had to be sent using 2 LMP messages, relevant bytes from this first LMP message are always passed on to the service function by \verb|rpc_handler_recv_closure|.

Another change introduced with the LMP protocol was, that now two bytes of the reserved 64 bytes of metadata per LMP message were used. Previously only one byte was used to send the message type. What changed is, that now one byte determines which RPC function the message belongs to and the other byte determines what kind of message is being sent. This allows each RPC call to have its own sub protocol for sending different kind of messages. For example in the listings above we can see that a different message type is used when sending the commandline.

\section{Bonus Objectives}

\subsection{Large Messages}
Sending large messages was implemented by sending consecutive LMP messages. We deliberated if we send frame capabilities and use shared memory to pass large messages. The advantage of sending large messages by sending frames would have been, that only one LMP message would have to be sent and therefore less context switches would be needed. Additional context switches are always taking away from the performance. The way LMP works at least 2 context switches per LMP message are needed when sending large messages using multiple LMP messages. One context switch for sending the LMP message and a context switch back to the sender so the next LMP message can be sent.

The disadvantage of using a frame is, that we considered it to be more complicated and time consuming to implement. Another disadvantage is that for messages that could be sent using a few LMP messages allocating a whole frame seemed to be a waste. It would be possible to create a system to reuse frames until the were filled up and then send new frames, but that would make the whole large message sending even more complicated.

The large message sending was integrated into \verb|lmp_protocol| too with the functions \verb|lmp_protocol_send_bytes_cap|, \verb|lmp_protocol_recv_bytes_cap|, \verb|lmp_protocol_send_string_cap| and \verb|lmp_protocol_recv_string_cap|. The send bytes function takes a pointer to a byte array and a size and sends in a first message the size in the fisrt 64 bytes and the initial 2 * 64 bytes in the remaining payload. Each of the following LMP messages contains 3 * 64 bytes until size bytes have been sent.

The receive bytes function works as a parallel to send bytes by allocating a buffer of the size that is sent with the initial message and then filling this buffer as bytes are being received over LMP messages.
The string sending and receiving works almost identical. The string send function takes a 0 terminated string and calculates the size of it and then proceeds to send the string by first sending the size and then all the bytes identically to how it was done for the send bytes function. Receiving the string works exactly the same as receiving the bytes, but the string receiving function does not return a size.

With both the bytes and the string functions for passing large messages there can also be sent a capability alongside. The capability is send with the first message that is passed. There is no guarantee that more than one message is passed, but always at least one message is passed. Therefore the interface allows to pass zero or one capability.

\section{Process Management}

TODO
(Does this belong here?)

\section{RPC Protocol}

TODO
